{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "- A network consists of $L$ layers, with $k_0$ units in the input layer and $k_1$ units in the hidden layers, where $l = 1, \\dots, L$\n",
    "\n",
    "- The input layer contains four units corresponding\n",
    "to the number of features used to represent a sample of this particular data\n",
    "- At the end of the network,\n",
    "we find the output layer, which corresponds to the network prediction for a given feature vector. In this example, the output layer has two units that could correspond to two classes in a classification problem.\n",
    "- Between the input and output layer, we find one or more hidden layers. These are responsible for mapping and transforming the input into a representation where the output layer can optimally perform the desired task.\n",
    "\n",
    "\n",
    "<span style=\"color: lightgreen\">Increasing</span> the number of <span style=\"color: orange\">units</span> is referred to as increasing the \"width\" of the network\n",
    "\n",
    "<span style=\"color: lightgreen\">Increasing</span> the number of <span style=\"color: orange\">layers</span> is referred to as increasing the \"depth\" of the network, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron\n",
    "For each neuron a weighted sum is computed by multiplying the output of the previous layer with the\n",
    "weight and bias of the current layer, that is:\n",
    "\n",
    "##### 1. Weighted sum calculation:\n",
    "\n",
    "- $z_j^{(l)}(i) = \\sum{}_{k = 1}^{k_{l-1}} w_jk^{(l)}a_k^{(l-1)}(i) + b_j^{(l)}$\n",
    "\n",
    "  ##### Where:\n",
    "  - $w_jk^{(l)}$: is the weight connecting neuron $k$ in layer $l - 1$ to neuron $j$ in layer $l$\n",
    "  - $a_k^{(l-1)}(i)$: output(activation) of neuron $k$ in layer $l - 1$ for sample $i$\n",
    "  - $b_j^{(l)}$: bias term for neuron $j$ in layer $l$\n",
    "  - $k_{l-1}$: number of neurons in layer $l - 1$\n",
    "  - $N$: number of samples\n",
    "\n",
    "\n",
    "##### 2. Applying the activation function:\n",
    "The weighted sum $z_j^{(l)}(i)$ is passed through an <span style=\"color: lightgreen\">activation function</span> $f(*)$, which introduces non-linearity:\n",
    "\n",
    "- $a_j^{(l)}(i) = f(z_j^{(l)}(i))$\n",
    "\n",
    "This determines the final output of neuron $j$ in layer $l$\n",
    "\n",
    "##### 3. Input and output cases:\n",
    "\n",
    "- Input layer ($l = 0$): The activations are simply the input features\n",
    "\n",
    "    - $a_k^{(0)}(i) = x_k(i) ,  k = 1, \\dots, k_0$\n",
    "- Output layer ($l = L$): The activations represent the final predictions:\n",
    "    - $a_k^{(L)}(i) = \\hat{y}_k(i) ,  k = 1, \\dots, k_L$\n",
    "\n",
    "  where $\\hat{y}_k(i)$ is the predicted output for sample $i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "\n",
    "### 1A) Derivation of backpropagation algorithm for a single sample \n",
    "\n",
    "Assume the sum of squared distances cost function:\n",
    "\n",
    "  - $C = \\frac{1}{2}\\sum{}_{j = 1}^{k_L}(\\hat{y_j} - y_j)^2$\n",
    "\n",
    "\n",
    "Show that the gradient with respect to the cost function for the weight connecting the $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
