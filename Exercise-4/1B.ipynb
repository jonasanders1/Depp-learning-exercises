{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "\n",
    "### 1B) PCA vs Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is PCA?\n",
    "- PCA is a technique used to reduce the number of dimensions (features) in the data while keeping as much important information as possible\n",
    "#### How does PCA work?\n",
    "- PCA finds new direction (Principle Components) in the data that capture the most variation. \n",
    "- These directions are like new axis (Think of it like rotating the graph to find the best angle to view the data).\n",
    "- You can then project your data onto these new axes to reduce its dimensionality.\n",
    "\n",
    "#### PCA math simplified...\n",
    "- The data is represented as vactirs $x^{(n)}$, where each vector had $d$ features\n",
    "- PCA represents each data point $x^{(n)}$ as a combination of:\n",
    "  - The mean vector ($\\tilde{x}$), which is just the average of all data points.\n",
    "  - A sum of projections onto new axis $(u_1, u_2,..., u_d)$, scaled by coefficients $z_i^{(n)}$.\n",
    "\n",
    "    $x^{(n)} = \\bar{x} + z_1^{(n)}u_1 + z_2^{(n)}u_2 + ... + z_d^{(n)}u_d $\n",
    "\n",
    "- To reduce dimensionality, PCA keeps only the first $M$ components (Where $M > d$) and drops the rest. This reconstruction data ${\\hat{x}}^{(n)}$ is:\n",
    "\n",
    "    ${\\hat{x}}^{(n)} = \\bar{x} + z_1^{(n)}u_1 + z_2^{(n)}u_2 + ... + z_M^{(n)}u_M $\n",
    "\n",
    "#### The goal of PCA \n",
    "- PCA tries to minimize the reconstruction error, which is the difference between the original data $x^{(n)}$ and the reconstructed data ${\\hat{x}}^{(n)}$\n",
    "- The error is measured using the squared difference:\n",
    "\n",
    "\n",
    "   #### $L = \\sum\\limits_{n} ||x^{(n)} - {\\hat{x}}^{(n)} ||^2$\n",
    "\n",
    "- If you use all d components ($M = d$), there’s no error because you’re keeping all the information. But if you drop some components ($M < d$), the error comes from the missing parts.\n",
    "\n",
    "\n",
    "#### How PCA Relates to Autoencoders\n",
    "- An autoencoder is a neural network that tries to compress and reconstruct data, similar to PCA. \n",
    "- The connection here is that a linear autoencoder (with no nonlinearities) behaves like PCA.\n",
    "\n",
    "  Both aim to find the best way to reduce dimensionality while minimizing the reconstruction error.\n",
    "\n",
    "\n",
    "#### The reconstruction Error Formula\n",
    "- The final formula calculates the error by:\n",
    "    - Projecting the difference between the original data and the mean (${\\bar{x}}$) onto the dropped components ($u_{M+1}, u_{M+3}, ... , u_d $)\n",
    "    - Squaring these projections and summing them up.\n",
    "\n",
    "      ####   $L = \\frac{1}{N}\\sum_{i=M+1}^{d}\\sum_{n=1}^{N} [(u_i^{(T)}(x^{(n)} - \\bar{x}))]^2$\n",
    "\n",
    "\n",
    "- This gives the total error caused by dropping the components beyond $M$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1B) Show that the loss (Reconstruction Error):\n",
    "### $L = \\frac{1}{N}\\sum_{i=M+1}^{d}\\sum_{n=1}^{N} [(x^{(n)} - \\bar{x})^Tu_i]^2$\n",
    "\n",
    "### also can be expressed as:\n",
    "\n",
    "### $L = \\sum_{i=M+1} u_i^{T} \\hat{\\sum{}}u_i$\n",
    "\n",
    "### where:\n",
    "\n",
    "### $\\hat{\\sum{}}_{ij} = \\frac{1}{N}\\sum_{n=1}^{N}(x_i^{(n)} - \\bar{x_i})(x_j^{(n)} - \\bar{x_j})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: lightgreen\">1B) Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Expand the Squared Term\n",
    "\n",
    "The term inside the loss function is: \n",
    "\n",
    "- $[(x^{(n)} - \\bar{x})^Tu_i]^2 $\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "- $[(x^{(n)} - \\bar{x})^Tu_i]^2 = [u_i^{T}(x^{(n)} - \\bar{x})]^2 $ \n",
    "\n",
    "\n",
    "Using the property of dot products, this is equivalent to:\n",
    "\n",
    "-  $[u_i^{T}(x^{(n)} - \\bar{x})]^2 = [u_i^{T}(x^{(n)} - \\bar{x})] [u_i^{T}(x^{(n)} - \\bar{x})] $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 2: Rewrite using matrix notation\n",
    "\n",
    "The squared term can be expressed as a quadratic form:\n",
    "\n",
    "- $[u_i^{T}(x^{(n)} - \\bar{x})]^2 = (n^{(n)} - \\bar{x})^Tu_i u_i^{T}(x^{(n)} - \\bar{x})$\n",
    "\n",
    "This is because: \n",
    "\n",
    "- $u_i^{T}(n^{(n)} - \\bar{x}) = (n^{(n)} - \\bar{x})^{T}u_i $\n",
    "\n",
    "and multiplying a scalar by itself is the same as squaring it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Sum Over All Data Points**\n",
    "Now, sum this term over all data points \\(n = 1, 2, \\dots, N\\):\n",
    "\n",
    "$\n",
    "\\sum_{n=1}^{N} \\left[ u_i^T (x^{(n)} - \\bar{x}) \\right]^2 = \\sum_{n=1}^{N} (x^{(n)} - \\bar{x})^T u_i u_i^T (x^{(n)} - \\bar{x}).\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Introduce the Covariance Matrix \\(\\hat{\\Sigma}\\)**\n",
    "The **covariance matrix** \\(\\hat{\\Sigma}\\) is defined as:\n",
    "\n",
    "$\n",
    "\\hat{\\Sigma} = \\frac{1}{N} \\sum_{n=1}^{N} (x^{(n)} - \\bar{x})(x^{(n)} - \\bar{x})^T.\n",
    "$\n",
    "\n",
    "Notice that the term \\((x^{(n)} - \\bar{x})^T u_i u_i^T (x^{(n)} - \\bar{x})\\) can be rewritten using the covariance matrix. Specifically:\n",
    "\n",
    "$\n",
    "\\sum_{n=1}^{N} (x^{(n)} - \\bar{x})^T u_i u_i^T (x^{(n)} - \\bar{x}) = N \\cdot u_i^T \\hat{\\Sigma} u_i.\n",
    "$\n",
    "\n",
    "This is because:\n",
    "$\n",
    "u_i^T \\hat{\\Sigma} u_i = \\frac{1}{N} \\sum_{n=1}^{N} u_i^T (x^{(n)} - \\bar{x})(x^{(n)} - \\bar{x})^T u_i.\n",
    "$\n",
    "\n",
    "Multiplying both sides by \\(N\\) gives:\n",
    "$\n",
    "N \\cdot u_i^T \\hat{\\Sigma} u_i = \\sum_{n=1}^{N} u_i^T (x^{(n)} - \\bar{x})(x^{(n)} - \\bar{x})^T u_i.\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Substitute Back into the Loss Formula**\n",
    "Now, substitute this result back into the reconstruction error formula:\n",
    "\n",
    "$\n",
    "L = \\frac{1}{N} \\sum_{i=M+1}^{d} \\sum_{n=1}^{N} \\left[ u_i^T (x^{(n)} - \\bar{x}) \\right]^2 = \\frac{1}{N} \\sum_{i=M+1}^{d} N \\cdot u_i^T \\hat{\\Sigma} u_i.\n",
    "$\n",
    "\n",
    "The \\(N\\) terms cancel out, leaving:\n",
    "\n",
    "$\n",
    "L = \\sum_{i=M+1}^{d} u_i^T \\hat{\\Sigma} u_i.\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Expression**\n",
    "Thus, the reconstruction error loss \\(L\\) can be expressed as:\n",
    "\n",
    "$\n",
    "L = \\sum_{i=M+1}^{d} u_i^T \\hat{\\Sigma} u_i,\n",
    "$\n",
    "\n",
    "where:\n",
    "- \\(u_i\\) are the dropped principal components (\\(i = M+1, M+2, \\dots, d\\)).\n",
    "- \\(\\hat{\\Sigma}\\) is the covariance matrix of the data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
