{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "\n",
    "### 1A) Autoencoder loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point, the exercises have been focused on supervised learning. In the coming weeks we will\n",
    "start to focus on unsupervised approaches, where this week’s exercise will be concentrated on autoencoders.\n",
    "Assume a dataset of N d-dimensional feature vectors, where the nth vector is represented as:\n",
    "**x(n) = [x(n)1 , x(n)2 , · · · , x(n)d ]T .**\n",
    "\n",
    "An autoencoder usually consists of two parts, an encoder and a decoder. The encoder is tasked with\n",
    "learning a useful representation of the data while the decoder tries to reconstruct the original input from the\n",
    "representation obtained by the encoder. Consider a simple autoencoder with one hidden layer consisting of\n",
    "M neurons, no bias and linear activation functions. For a single sample, the encoder of this model can be\n",
    "described by\n",
    "\n",
    "### Encoder:\n",
    "### **h(x(n)) = Wx(n) = c(n)**\n",
    "\n",
    "where:\n",
    "- **h(x(n))** : The **encoder function** which takes in the **Input vector (x(m))**\n",
    "- **X(n)**    : **Input vector**. Its a **d-dimensional** vector (d-features)\n",
    "- **W**       : is the **weight matrix**\n",
    "- **Wx(n)**   : is the **matrix multiplication** of the **weights** and the **Input vector**\n",
    "- **c(n)**    : is the **compressed representation**\n",
    "\n",
    "\n",
    "\n",
    "### Decoder:\n",
    "### **g(c(n)) = W∗c(n) =  ̃x(n)**\n",
    "\n",
    "where:\n",
    "- **g(c(n))** : The decoder function which takes in the compressed representation and tries to reconstruct the original input.\n",
    "-  **c(n)**   : Compressed representation\n",
    "- W*          : It the weight matrix of the decoder W* = W^T\n",
    "- W∗c(n)      : It is the matrix multiplication of the weight matrix (W*) and the compressed representation **(c(n))**\n",
    "-  ̃x(n)       : It is the reconstructed output of the autoencoder\n",
    "\n",
    "\n",
    "A common type of regularization for autoencoders is so called tied-weights. For the autoencoder described\n",
    "above, this regularization can be described by setting W∗ = WT , thus limiting the capacity of the autoencoder\n",
    "and reducing the potential for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Example:\n",
    " \n",
    " - A 3-dimensional vector(d = 3):  $x^{(n)} = [1, 2, 3]$\n",
    " - Two neurons in the hidden layer (M = 2), so W is a 2 x 3 matrix: $W = \\begin{bmatrix} 0.5 & 0.1 & 0.3 \\\\ 0.2 & 0.4 & 0.6 \\end{bmatrix}$\n",
    " \n",
    " - Multiplying W by x(n) gives: \n",
    " $c^{(n)} = Wx^{(n)} = \\begin{bmatrix} 0.5 \\cdot 1 + 0.1 \\cdot 2 + 0.3 \\cdot 3 \\\\ 0.2 \\cdot 1 + 0.4 \\cdot 2 + 0.6 \\cdot 3 \\end{bmatrix}$\n",
    "                    \n",
    " $c^{(n)} = \\begin{bmatrix} 1.6 \\\\ 2.8 \\end{bmatrix}$\n",
    "\n",
    " ### Decoder Example:\n",
    " - Given that the Encoder gave compressed representation:  **$c^{(n)} = \\begin{bmatrix} 1.6 \\\\ 2.8 \\end{bmatrix}$** \n",
    " - The decoder weight matrix $W^* = W^T$ (Same as Encoder weights, but Transposed):\n",
    "\n",
    "- $W^* = W^T =  \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.1 & 0.4 \\\\ 0.3 & 0.6 \\end{bmatrix}$\n",
    "- To reconstruct, we have to Multiply the $W^*$ by $c^{(n)}$:\n",
    "- $\\tilde{x}^{(n)} = W^*C^{(n)} =  \\begin{bmatrix} 0.5 * 1.6 + 0.2 * 2.8 \\\\0.1 * 1.6 + 0.4 * 2.8\\\\ 0.3 * 1.6 + 0.6 * 2.8\\end{bmatrix} = \\begin{bmatrix} 1.36 \\\\ 1.28 \\\\ 2.16 \\end{bmatrix}$\n",
    "\n",
    "- So, $\\tilde{x}^{(n)} = \\begin{bmatrix} 1.36 \\\\ 1.28 \\\\ 2.16 \\end{bmatrix}$ is the reconstructed version of the original input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: lightgreen\">1A) Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming a mean squared error loss function and tied-weights, what would be the loss function of the autoencoder described above? \n",
    "\n",
    "- Mean Squared Error(MSE) calculates the average squared difference between the original input ($x^{(n)}$) and the reconstructed output $\\tilde{x}^{(n)}$ \n",
    "\n",
    "- For a single datapoint ($x^{(n)}$), the squared error is:\n",
    "\n",
    "$Squared Error = ||x^{(n)} - \\tilde{x}^{(n)}||^2$\n",
    "\n",
    "- Since the autoencoder uses tied weights ($W* = W^T$), the reconstructed output $\\tilde{x}^{(n)}$ is:\n",
    "\n",
    "$\\tilde{x}^{(n)} = g(c^{(n)}) = W*c^{(n)} = W^Tc^{(n)}$ \n",
    "\n",
    "and $c^{(n)} = h(x^{(n)})= Wx^{(n)}$\n",
    "\n",
    "- Substituting $c^{(n)}$ into the reconstruction:\n",
    "$\\tilde{x}^{(n)} = W^T(Wx^{(n)})$\n",
    "\n",
    "So, the MSE loss function for the entire dataset of $N$ samples is:\n",
    "  \n",
    "$Loss = \\frac{1}{N}\\sum_{n=1}^{N} ||x^{(n)} - W^T(Wx^{(n)})||^2$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a) Assuming d > M , what do we call such an autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input dimension $d$ is greater than the number of hidden neurons $M (d > M)$, the autoencoder is called an **undercomplete** autoencoder.\n",
    "\n",
    "#### Why?\n",
    "The hidden layer har fewer neurons ($M$) than the input dimension ($d$), which forces he autoencoder to learn a compressed representation of the data. This compression helps the model focus on the most important features of the data, making it useful for tasks like dimensionality reduction or feature extraction.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
